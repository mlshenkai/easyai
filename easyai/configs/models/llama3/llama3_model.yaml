model:
  arch: llama3_model_causal_lm
  model_type: pretrain
  config:
    config:
      attention_bias: False
      attention_dropout: 0.0
      bos_token_id: 128000
      eos_token_id: 128001
      hidden_act: silu
      hidden_size: 4096
      initializer_range: 0.02
      intermediate_size: 14336
      max_position_embeddings: 8192
      model_type: llama
      num_attention_heads: 32
      num_hidden_layers: 32
      num_key_value_heads: 8
      pretraining_tp: 1
      rms_norm_eps: 1e-5
      rope_scaling: null
      rope_theta: 500000.0
      tie_word_embeddings: False
      torch_dtype: bfloat16
      use_cache: True
      vocab_size: 128256
      name_or_path: /code-online/modelscope/llama3-chinese-Instruct/model_hub