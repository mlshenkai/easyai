{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, Blip2Model, Blip2Config, Blip2PreTrainedModel, Blip2VisionConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.activations import ACT2FN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import math\n",
    "from easyai.common.tensor_utils import concat_all_gather\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T09:51:55.231785Z",
     "start_time": "2024-05-16T09:51:55.226193Z"
    }
   },
   "id": "d9006f88919a24ea",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "992af2e7d1aa6628"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2VisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.config = config\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.embed_dim = config.hidden_size\n",
    "        \n",
    "        self.class_embeds = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "        \n",
    "        self.patch_embeds = nn.Conv2d(\n",
    "            in_channels=3, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size\n",
    "        )\n",
    "        \n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches + 1\n",
    "        \n",
    "        self.position_embeds = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n",
    "    \n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        :param pixel_values: [batch_size, 3, image_size, image_size]\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        target_dtype = self.patch_embeds.weight.dtype\n",
    "        \n",
    "        patch_embeds = self.patch_embeds(pixel_values.to(target_dtype))  # [batch_size, embed_dim, patch_size, patch_size]\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(2, 1)  # [batch_size, patch_size*patch_size, embed_dim]\n",
    "        \n",
    "        class_embeds = self.class_embeds.expand(batch_size, 1, -1)\n",
    "        \n",
    "        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)  # [batch_size, patch_size*patch_size+1, embed_dim]\n",
    "        \n",
    "        embeddings = embeddings + self.position_embeds[:, :embeddings.size(1), :].to(target_dtype)\n",
    "        return embeddings\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:44:52.554023Z",
     "start_time": "2024-05-16T08:44:52.546108Z"
    }
   },
   "id": "2d215ccf36936110",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class Blip2MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5  # sqrt(head_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "        \n",
    "        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)\n",
    "        \n",
    "        if config.qkv_bias:\n",
    "            q_bias = nn.Parameter(torch.zeros(self.embed_dim))\n",
    "            v_bias = nn.Parameter(torch.zeros(self.embed_dim))\n",
    "        else:\n",
    "            q_bias = None\n",
    "            v_bias = None\n",
    "        \n",
    "        if q_bias is not None:\n",
    "            qkv_bias = torch.cat([q_bias, torch.zeros_like(v_bias, requires_grad=False), v_bias])\n",
    "            self.qkv.bias = nn.Parameter(q_bias)\n",
    "        \n",
    "        self.projector = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "    \n",
    "    \n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, batch_size: int):\n",
    "        return tensor.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = None, output_attentions: Optional[bool] = False) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \n",
    "        batch_size, seq_length, embed_dim = hidden_states.size()\n",
    "        \n",
    "        mixed_qkv = self.qkv(hidden_states)  # [batch_size, seq_length, 3*embed_dim]\n",
    "        mixed_qkv = mixed_qkv.reshape(batch_size, seq_length, 3, self.num_heads, embed_dim // self.num_heads).permute(\n",
    "            2, 0, 3, 1, 4\n",
    "        )  # [3, batch_size, num_heads, seq_length, head_dim]\n",
    "        \n",
    "        query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]\n",
    "        \n",
    "        \n",
    "        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))  # [batch_size, num_heads, seq_length, seq_length]\n",
    "        \n",
    "        attention_scores = attention_scores * self.scale \n",
    "        \n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_states)  # [batch_size, num_heads, seq_length, head_dim]\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3)  # [batch_size, seq_length, num_heads, head_dim]\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim, )  # [batch_size, seq_length] + [embed_dim, ] = [batch_size, seq_length, embed_dim]\n",
    "        context_layer = context_layer.reshape(new_context_layer_shape)  # [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        output = self.projector(context_layer)\n",
    "        outputs = (output, attention_probs) if output_attentions else (output, )\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T09:49:06.414440Z",
     "start_time": "2024-05-16T09:49:06.409025Z"
    }
   },
   "id": "d4f86209246468b9",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.activation_fn = ACT2FN[config.hidden_act]\n",
    "        \n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T09:53:48.988253Z",
     "start_time": "2024-05-16T09:53:48.983416Z"
    }
   },
   "id": "7422578a166b05ff",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = Blip2MultiHeadAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = Blip2MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, output_attentions: Optional[bool] = False):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states, attn_weights = self.self_attn(hidden_states, head_mask=attention_mask, output_attentions=output_attentions)\n",
    "        hidden_states = hidden_states + residual\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        output = hidden_states+residual\n",
    "        outputs = (output, attn_weights) if output_attentions else (output, )\n",
    "        return outputs\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T06:49:19.869278Z",
     "start_time": "2024-05-17T06:49:19.865040Z"
    }
   },
   "id": "2bb4651d2a15534c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Blip2Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Blip2EncoderLayer(config)\n",
    "                for _ in range(config.num_hidden_layers)\n",
    "            ] \n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_embeds, \n",
    "        attention_mask: Optional[torch.Tensor]=None, \n",
    "        output_attentions: Optional[bool]=None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        \n",
    "        hidden_states = input_embeds\n",
    "        for idx, encoder_layer_module in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states, )\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*input):\n",
    "                        return module(*input, output_attentions)\n",
    "                    return custom_forward\n",
    "                \n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward,\n",
    "                    hidden_states,\n",
    "                    attention_mask\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer_module(hidden_states, attention_mask, output_attentions=output_attentions)\n",
    "            \n",
    "            hidden_states = layer_outputs[0]\n",
    "            \n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1], )\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states, )\n",
    "        \n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, encoder_states, all_attentions] if v is not None\n",
    "            )\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=encoder_states,\n",
    "            attentions=all_attentions\n",
    "        )\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3039a207db039b22"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "\n",
    "\n",
    "class Blip2VisionModel(Blip2PreTrainedModel):\n",
    "    main_input_name = \"pixel_values\"\n",
    "    config_class = Blip2VisionConfig\n",
    "    \n",
    "    def __init__(self, config: Blip2VisionConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = Blip2VisionEmbeddings(config)\n",
    "        self.encoder = Blip2Encoder(config)\n",
    "        self.post_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        \n",
    "        encoder_outputs = self.encoder(\n",
    "            input_embeds = hidden_states,\n",
    "            output_attentions = output_attentions,\n",
    "            output_hidden_states = output_hidden_states,\n",
    "            return_dict = return_dict\n",
    "        )\n",
    "        last_hidden_states = encoder_outputs[0]\n",
    "        last_hidden_states = self.post_layer_norm(last_hidden_states)\n",
    "        \n",
    "        pooled_output = last_hidden_states[:, 0, :]\n",
    "        pooled_output = self.post_layer_norm(pooled_output)\n",
    "        \n",
    "        if not return_dict:\n",
    "            return (last_hidden_states, pooled_output) + encoder_outputs[1:]\n",
    "        \n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_states,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.embeddings\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c465afb38c73b92"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2QFormerMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        \"\"\"\n",
    "        multi head attention same bert-self-attention\n",
    "        :param config: \n",
    "        :param is_cross_attention: \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        if is_cross_attention:\n",
    "            self.key = nn.Linear(config.encoder_hidden_size, self.all_head_size)\n",
    "            self.value = nn.Linear(config.encoder_hidden_size, self.all_head_size)\n",
    "        else:\n",
    "            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "        self.save_attention = False\n",
    "\n",
    "    def save_attn_gradients(self, attn_gradients):\n",
    "        self.attn_gradients = attn_gradients\n",
    "\n",
    "    def get_attn_gradients(self):\n",
    "        return self.attn_gradients\n",
    "\n",
    "    def save_attention_map(self, attention_map):\n",
    "        self.attention_map = attention_map\n",
    "\n",
    "    def get_attention_map(self):\n",
    "        return self.attention_map\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            head_mask=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            past_key_value=None,\n",
    "            output_attentions=False,\n",
    "    ):\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        if is_cross_attention and self.save_attention:\n",
    "            self.save_attention_map(attention_probs)\n",
    "            attention_probs.register_hook(self.save_attn_gradients)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs_dropped = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs_dropped = attention_probs_dropped * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "            \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:38:44.332856Z",
     "start_time": "2024-05-17T09:38:44.267682Z"
    }
   },
   "id": "44c331606cfcf8f7",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2QFormerSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        add+norm\n",
    "        \n",
    "        :param config: \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69534c3b3563fee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n",
    "\n",
    "\n",
    "class Blip2QFormerAttention(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.attention = Blip2QFormerMultiHeadAttention(config, is_cross_attention)\n",
    "        self.output = Blip2QFormerSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.attention.query = prune_linear_layer(self.attention.query, index)\n",
    "        self.attention.key = prune_linear_layer(self.attention.key, index)\n",
    "        self.attention.value = prune_linear_layer(self.attention.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n",
    "        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        self_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:55:11.214678Z",
     "start_time": "2024-05-17T09:55:11.198178Z"
    }
   },
   "id": "b51bd67ef188f159",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2QFormerIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f89cd62f97a4477"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Blip2QFormerOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66db18255d813a1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import apply_chunking_to_forward\n",
    "\n",
    "\n",
    "class Blip2QFormerLayer(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = Blip2QFormerAttention(config)\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        if layer_idx % config.cross_attention_frequency == 0:\n",
    "            self.crossattention = Blip2QFormerAttention(config, is_cross_attention=True)\n",
    "            self.has_cross_attention = True\n",
    "        else:\n",
    "            self.has_cross_attention = False\n",
    "\n",
    "        self.intermediate_query = Blip2QFormerIntermediate(config)\n",
    "        self.output_query = Blip2QFormerOutput(config)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            head_mask=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            past_key_value=None,\n",
    "            output_attentions=False,\n",
    "            query_length=0,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:-1]\n",
    "\n",
    "        present_key_value = self_attention_outputs[-1]\n",
    "\n",
    "        if query_length > 0:\n",
    "            query_attention_output = attention_output[:, :query_length, :]\n",
    "\n",
    "            if self.has_cross_attention:\n",
    "                if encoder_hidden_states is None:\n",
    "                    raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n",
    "                cross_attention_outputs = self.crossattention(\n",
    "                    query_attention_output,\n",
    "                    attention_mask,\n",
    "                    head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "                query_attention_output = cross_attention_outputs[0]\n",
    "                # add cross attentions if we output attention weights\n",
    "                outputs = outputs + cross_attention_outputs[1:-1]\n",
    "\n",
    "            layer_output = apply_chunking_to_forward(\n",
    "                self.feed_forward_chunk_query,\n",
    "                self.chunk_size_feed_forward,\n",
    "                self.seq_len_dim,\n",
    "                query_attention_output,\n",
    "            )\n",
    "\n",
    "            if attention_output.shape[1] > query_length:\n",
    "                layer_output_text = apply_chunking_to_forward(\n",
    "                    self.feed_forward_chunk,\n",
    "                    self.chunk_size_feed_forward,\n",
    "                    self.seq_len_dim,\n",
    "                    attention_output[:, query_length:, :],\n",
    "                )\n",
    "                layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n",
    "        else:\n",
    "            layer_output = apply_chunking_to_forward(\n",
    "                self.feed_forward_chunk,\n",
    "                self.chunk_size_feed_forward,\n",
    "                self.seq_len_dim,\n",
    "                attention_output,\n",
    "            )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "    def feed_forward_chunk_query(self, attention_output):\n",
    "        intermediate_output = self.intermediate_query(attention_output)\n",
    "        layer_output = self.output_query(intermediate_output, attention_output)\n",
    "        return layer_output\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "593d5400d39f3f42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from transformers import logger\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "\n",
    "class Blip2QFormerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList(\n",
    "            [Blip2QFormerLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            head_mask=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            past_key_values=None,\n",
    "            use_cache=None,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "            query_length=0,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            layer_module = self.layer[i]\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions, query_length)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                    query_length,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if layer_module.has_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3a43971a48ebad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Blip2QFormerConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "\n",
    "class Blip2QFormerModel(Blip2PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Querying Transformer (Q-Former), used in BLIP-2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Blip2QFormerConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.encoder = Blip2QFormerEncoder(config)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def get_extended_attention_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_shape: Tuple[int],\n",
    "        device: torch.device,\n",
    "        has_query: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device (`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - the model is an encoder, so make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_embeds: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n",
    "            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n",
    "            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n",
    "            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n",
    "            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n",
    "            `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, `optional`):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = (\n",
    "            past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n",
    "        )\n",
    "\n",
    "        query_length = query_embeds.shape[1] if query_embeds is not None else 0\n",
    "\n",
    "        embedding_output = self.layernorm(query_embeds)\n",
    "        embedding_output = self.dropout(embedding_output)\n",
    "\n",
    "        input_shape = embedding_output.size()[:-1]\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = embedding_output.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if encoder_hidden_states is not None:\n",
    "            if type(encoder_hidden_states) == list:\n",
    "                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n",
    "            else:\n",
    "                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "\n",
    "            if type(encoder_attention_mask) == list:\n",
    "                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n",
    "            elif encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "            else:\n",
    "                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            query_length=query_length,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12a8f3dafdb4830f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertTokenizer\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mQFormerEmbedding\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config: Blip2QFormerConfig):\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class QFormerEmbedding(nn.Module):\n",
    "    def __init__(self, config: Blip2QFormerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id\n",
    "        )\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1))\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        query_embeddings: Optional[torch.Tensor] = None,\n",
    "        past_seq_length: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.size(1) if input_ids is not None else 0\n",
    "        embeddings = query_embeddings\n",
    "        if input_ids is not None:\n",
    "            if position_ids is None:\n",
    "                position_ids = self.position_ids[:, past_seq_length: seq_length + past_seq_length].clone()\n",
    "            word_embeddings = self.token_embedding(input_ids)\n",
    "            position_embeddings = self.position_embeddings(position_ids.long())\n",
    "            embeddings = word_embeddings + position_embeddings\n",
    "            \n",
    "            if query_embeddings is not None:\n",
    "                embeddings = torch.cat([query_embeddings, embeddings], dim=1)\n",
    "        assert isinstance(embeddings, torch.Tensor)\n",
    "        return embeddings\n",
    "\n",
    "class QFormerPredictionHead(nn.Module):\n",
    "    def __init__(self, config: Blip2QFormerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activate_fn = nn.GELU()\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dense2 = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "    \n",
    "    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(sequence_output)\n",
    "        hidden_states = self.activate_fn(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Blip2QFormerForCLM(Blip2PreTrainedModel):\n",
    "    def __init__(self, config: Blip2Config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.vision_embeddings = Blip2VisionModel(config.vision_config)\n",
    "        self.qformer_model = Blip2QFormerModel(config.qformer_config)\n",
    "        # self.qformer_embeddings = QFormerEmbedding(config.qformer_config)\n",
    "        self.itm_head = nn.Linear(config.qformer_config.hidden_size, 2)\n",
    "        # self.tokenizer = self.init_tokenizer()\n",
    "        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n",
    "        self.query_tokens.data.normal_(mean=0.0, std=config.qformer_config.initializer_range)\n",
    "        self.vision_projector = nn.Linear(config.qformer_config.hidden_size, config.qformer_config.hidden_size)\n",
    "        self.text_projector = nn.Linear(config.qformer_config.hidden_size, config.qformer_config.hidden_size)\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        image_ids: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ):\n",
    "        \n",
    "        image_embeds_outputs = self.vision_embeddings(\n",
    "            pixel_values = pixel_values,\n",
    "            output_attentions = output_attentions,\n",
    "            output_hidden_states = output_hidden_states,\n",
    "            return_dict = return_dict\n",
    "        )\n",
    "        image_embeds = image_embeds_outputs.last_hidden_state  # [batch_size, num_query_tokens, hidden_size]\n",
    "        image_attention_mask = torch.ones(size=image_embeds.size()[:-1], dtype=pixel_values.dtype).to(pixel_values.device)\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        query_tokens = self.query_tokens.expand(batch_size, -1, -1)\n",
    "        \n",
    "        query_output = self.qformer_model(\n",
    "            query_embeds = query_tokens,\n",
    "            encoder_hidden_states = image_embeds,\n",
    "            encoder_attention_mask = image_attention_mask,\n",
    "            use_cache = use_cache,\n",
    "            output_attentions = output_attentions,\n",
    "            output_hidden_states = output_hidden_states,\n",
    "            return_dict = return_dict,\n",
    "        )  # [batch_size, num_query_tokens, hidden_size]\n",
    "        # [batch_size, num_query_tokens, hidden_size]\n",
    "        image_feat = F.normalize(\n",
    "            self.vision_projector(query_output.last_hidden_state), \n",
    "            dim=-1\n",
    "        )  # [batch_size, num_query_tokens, hidden_size]\n",
    "        \n",
    "        text_output = self.qformer_model(\n",
    "            query_embeds = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache = use_cache,\n",
    "            output_attentions = output_attentions,\n",
    "            output_hidden_states = output_hidden_states,\n",
    "            return_dict = return_dict,\n",
    "        )  # [batch_size, [CLS]+seq_length, hidden_size] \n",
    "        text_feat = F.normalize(\n",
    "            self.text_projector(text_output.last_hidden_state[:, 0, :]),\n",
    "            dim=-1\n",
    "        )  # [batch_size, [CLS]+seq_length, hidden_size] ->\n",
    "        # [batch_size, hidden_size] -> [batch_size, hidden_size]\n",
    "        \n",
    "        # Image Text Contrastive (IMT loss)\n",
    "        image_feat_all = concat_all_gather(image_feat) # [batch_size, num_query_tokens, hidden_size]\n",
    "        text_feat_all = concat_all_gather(text_feat)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # \n",
    "        \n",
    "        sim_q2t = torch.matmul(\n",
    "            image_feat.unsqueeze(1), text_feat_all.unsqueeze(-1)\n",
    "        )  # [batch_size, 1, num_query_tokens, hidden_size] * [batch_size, hidden_size, 1] => [batch_size, batch_size, num_query_tokens, 1]\n",
    "        sim_q2t = sim_q2t.squeeze() # [batch_size, batch_size, num_query_tokens, 1] => [batch_size, batch_size, num_query_tokens]\n",
    "        \n",
    "        # image-text similar\n",
    "        sim_i2t, _ = sim_q2t.max(-1)\n",
    "        sim_i2t = sim_i2t / self.config.qformer_config.temperature\n",
    "        \n",
    "        sim_t2q = torch.matmul(\n",
    "            text_feat.unsqueeze(1).unsqueeze(1), image_feat_all.permute(0, 2, 1)\n",
    "        )  # [batch_size, 1, 1, hidden_size] * [batch_size, hidden_size, num_query_tokens] => [batch_size, 1, batch_size, num_query_tokens]\n",
    "        sim_t2q = sim_t2q.squeeze()\n",
    "        \n",
    "        sim_t2i, _ = sim_t2q.max(-1)\n",
    "        sim_t2i = sim_t2i / self.config.qformer_config.temperature\n",
    "        \n",
    "        rank = dist.get_rank()\n",
    "        targets = torch.linspace(rank * batch_size, rank*batch_size + batch_size - 1, batch_size, dtype=int).to(pixel_values.device)\n",
    "        \n",
    "        if image_ids is not None:\n",
    "            image_ids = image_ids.view(-1, 1)\n",
    "            image_ids_all = concat_all_gather(image_ids)\n",
    "            pos_idx = torch.eq(image_ids, image_feat_all.t()).float()\n",
    "            sim_targets = pos_idx / pos_idx.sum(1, keepdim=True)\n",
    "            sim_targets = 0.9 * sim_targets + 0.1 * torch.ones_like(sim_targets) / sim_targets.size(1)\n",
    "\n",
    "            loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_targets,dim=1).mean()\n",
    "            loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_targets,dim=1).mean()\n",
    "            loss_itc = (loss_t2i+loss_i2t)/2\n",
    "        else:\n",
    "            loss_itc = (\n",
    "                           F.cross_entropy(sim_i2t, targets, label_smoothing=0.1)\n",
    "                           + F.cross_entropy(sim_t2i, targets, label_smoothing=0.1)\n",
    "                       ) / 2\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T02:40:04.730566Z",
     "start_time": "2024-05-21T02:40:02.385556Z"
    }
   },
   "id": "3944ffa871dedf09",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Blip2Model(Blip2PreTrainedModel):\n",
    "    config_class = Blip2Config\n",
    "    main_input_name = \"pixel_values\"\n",
    "    def __init__(self, config: Blip2Config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        self.query_tokens = nn.Parameter(torch.ones(1, config.num_query_tokens, config.qformer_config.hidden_size))\n",
    "        self.qformer = Blip2QFormerModel(config.qformer_config)\n",
    "        self.vision_model = Blip2VisionModel(config.vision_config)\n",
    "        \n",
    "        self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n",
    "        if config.use_decoder_only_language_model:\n",
    "            language_model = AutoModelForCausalLM(config.text_config)\n",
    "        else:\n",
    "            language_model = AutoModelForSeq2SeqLM(config.text_config)\n",
    "        \n",
    "        if language_model._tied_weights_keys is not None:\n",
    "            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n",
    "        \n",
    "        self.language_model = language_model\n",
    "        \n",
    "        self.post_init()\n",
    "    \n",
    "    \n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.language_model.get_input_embeddings()\n",
    "    \n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        self.language_model.input_embeddings = value\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.language_model.set_output_embeddings(new_embeddings)\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        return self.language_model.get_output_embeddings()\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.language_model.get_encoder()\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.language_model.get_decoder()\n",
    "\n",
    "    def _tie_weights(self):\n",
    "        if not self.config.use_decoder_only_language_model:\n",
    "            self.language_model.encoder.embed_tokens = self.language_model.shared\n",
    "            self.language_model.decoder.embed_tokens = self.language_model.shared\n",
    "    \n",
    "    def get_text_features(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.Tensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if self.config.use_decoder_only_language_model:\n",
    "            text_outputs = self.language_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict\n",
    "            )\n",
    "        else:\n",
    "            input_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
    "            text_outputs = self.language_model(\n",
    "                input_embeds = input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                labels=labels\n",
    "            )\n",
    "        return text_outputs\n",
    "        \n",
    "    \n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values = pixel_values,\n",
    "            output_attentions = output_attentions,\n",
    "            output_hidden_states = output_hidden_states,\n",
    "            return_dict = return_dict\n",
    "        )\n",
    "        return vision_outputs\n",
    "    \n",
    "    \n",
    "    def get_qformer_features(self, pixel_values: Optional[torch.FloatTensor], out):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9966e91bce4ac2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
