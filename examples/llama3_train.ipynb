{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T11:55:30.198122Z",
     "start_time": "2024-06-28T11:55:30.190663Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('/code-online/code/easy_ai')\n",
    "import os\n",
    "os.environ[\"GRADIO_SERVER_PORT\"] = \"7474\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:13:15.225075Z",
     "start_time": "2024-06-28T12:13:15.216837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from easyai.configs import get_train_args\n",
    "\n",
    "sys.argv = [\"cli\", \"train\", \"--stage\", \"sft\", \"--do_train\", \"True\", \"--model_name_or_path\", \"/code-online/modelscope/llama3-chinese-Instruct\", \"--output_dir\", \"/code/logs\", \"--template\", \"default\", \"--dataset_dir\", \"/code-online/code/easy_ai/data\", \"--dataset\", \"alpaca_zh_demo\", \"--finetuning_type\", \"lora\"]"
   ],
   "id": "7e5be36fde3ee41f",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:13:16.182565Z",
     "start_time": "2024-06-28T12:13:15.958509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys.argv.pop(1)\n",
    "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args()"
   ],
   "id": "e6904c1d83861871",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2052] 2024-06-28 20:13:15,991 >> PyTorch: setting up devices\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO|training_args.py:1727] 2024-06-28 20:13:16,164 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2024 20:13:16 - WARNING - easyai.configs.parser - We recommend enable mixed precision training.\n",
      "06/28/2024 20:13:16 - INFO - easyai.configs.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:14:58.349725Z",
     "start_time": "2024-06-28T12:14:58.340466Z"
    }
   },
   "cell_type": "code",
   "source": "finetuning_args",
   "id": "617c1c4ea4d8f417",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinetuningArguments(use_badam=False, badam_mode='layer', badam_start_block=None, badam_switch_mode='ascending', badam_switch_interval=50, badam_update_ratio=0.05, badam_mask_mode='adjacent', badam_verbose=0, use_galore=False, galore_target=['all'], galore_rank=16, galore_update_interval=200, galore_scale=0.25, galore_proj_type='std', galore_layerwise=False, pref_beta=0.1, pref_ftx=0.0, pref_loss='sigmoid', dpo_label_smoothing=0.0, kto_chosen_weight=1.0, kto_rejected_weight=1.0, simpo_gamma=0.5, ppo_buffer_size=1, ppo_epochs=4, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_adapters=None, ref_model_quantization_bit=None, reward_model=None, reward_model_adapters=None, reward_model_quantization_bit=None, reward_model_type='lora', additional_target=None, lora_alpha=16, lora_dropout=0.0, lora_rank=8, lora_target=['all'], loraplus_lr_ratio=None, loraplus_lr_embedding=1e-06, use_rslora=False, use_dora=False, pissa_init=False, pissa_iter=4, pissa_convert=False, create_new_adapter=False, freeze_trainable_layers=2, freeze_trainable_modules=['all'], freeze_extra_modules=None, pure_bf16=False, stage='sft', finetuning_type='lora', use_llama_pro=False, freeze_vision_tower=True, train_mm_proj_only=False, plot_loss=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:15:47.954943Z",
     "start_time": "2024-06-28T12:15:47.945531Z"
    }
   },
   "cell_type": "code",
   "source": "training_args",
   "id": "fb572759d52079f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTrainingArguments(output_dir='/code/logs', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/code/logs/runs/Jun28_20-13-15_sk', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='/code/logs', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy=None, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:15:48.441829Z",
     "start_time": "2024-06-28T12:15:48.435666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from easyai.data import get_dataset\n",
    "from easyai.models import load_tokenizer, load_model\n"
   ],
   "id": "4d0b95df0487c0ef",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:15:49.146283Z",
     "start_time": "2024-06-28T12:15:49.137833Z"
    }
   },
   "cell_type": "code",
   "source": "data_args",
   "id": "2f675d44401d4f1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataArguments(template='default', dataset='alpaca_zh_demo', dataset_dir='/code-online/code/easy_ai/data', split='train', cutoff_len=1024, reserved_label_len=1, train_on_prompt=False, streaming=False, buffer_size=16384, mix_strategy='concat', interleave_probs=None, overwrite_cache=False, preprocessing_num_workers=None, max_samples=None, eval_num_beams=None, ignore_pad_token_for_loss=True, val_size=0.0, packing=False, tokenized_path=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:15:50.260749Z",
     "start_time": "2024-06-28T12:15:49.855351Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = load_tokenizer(model_args)",
   "id": "a459c172f1982f6f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2106] 2024-06-28 20:15:49,862 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-06-28 20:15:49,864 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-06-28 20:15:49,865 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-06-28 20:15:49,866 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-28 20:15:50,230 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:15:52.416361Z",
     "start_time": "2024-06-28T12:15:50.981456Z"
    }
   },
   "cell_type": "code",
   "source": "datasets = get_dataset(model_args=model_args, data_args=data_args,training_args=training_args,stage=finetuning_args.stage, **tokenizer)\n",
   "id": "514f2b25abd3ea10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2024 20:15:50 - INFO - easyai.data.loader - Loading dataset alpaca_zh_demo.json...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a38e1fa74694ea28d4864b4438a9f43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[35075, 25, 102227, 228, 64022, 64026, 50338, 69962, 90112, 23187, 45277, 105363, 110835, 108396, 121050, 5232, 125951, 121050, 34208, 9080, 64209, 37687, 9174, 72803, 25, 220, 125951, 121050, 21043, 112027, 108396, 124434, 121050, 115973, 112403, 56438, 118958, 33014, 108619, 68171, 50034, 31809, 9554, 108729, 24946, 24186, 8713, 125951, 32938, 78935, 13153, 110477, 21043, 112027, 48864, 124434, 113879, 121050, 3922, 112403, 125951, 21043, 56438, 112027, 9554, 108729, 113520, 34208, 99480, 76417, 3922, 56438, 9554, 112027, 108619, 68171, 48044, 58291, 43240, 19483, 125951, 41127, 13153, 3922, 125951, 122332, 68438, 125951, 17620, 117068, 116498, 116879, 125951, 110477, 15120, 121050, 68171, 103013, 249, 23187, 39013, 242, 5486, 102923, 108889, 32943, 34208, 105686, 110905, 35304, 10750, 24, 8107, 61075, 33671, 118664, 3490, 9080, 64209, 37687, 21043, 64467, 101402, 83175, 21043, 101402, 83175, 39276, 9554, 104356, 104587, 103179, 37687, 3922, 23039, 78519, 105885, 12774, 243, 101402, 83175, 117647, 47770, 9554, 121050, 1811, 103624, 121050, 76537, 105150, 35287, 42783, 55758, 126315, 64209, 37687, 103276, 28542, 3922, 112403, 127960, 64026, 103668, 110164, 121040, 9554, 104356, 1811, 9080, 64209, 127259, 118664, 30046, 21043, 103420, 102491, 73325, 14260, 105736, 101828, 103420, 102981, 19000, 845, 101083, 108142, 85707, 29391, 21405, 35287, 106616, 102700, 19967, 28038, 36827, 33014, 117238, 68931, 88343, 33443, 238, 43032, 35287, 101402, 83175, 39276, 23039, 78519, 105885, 12774, 243, 101402, 83175, 117238, 9554, 123123, 118187, 36827, 122411, 9554, 106246, 102210, 113433, 109098, 27384, 13647, 94, 112500, 1811, 128001]\n",
      "inputs:\n",
      "Human: 识别并解释给定列表中的两个科学理论：细胞理论和日心说。\n",
      "Assistant: 细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
      "\n",
      "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|end_of_text|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 125951, 121050, 21043, 112027, 108396, 124434, 121050, 115973, 112403, 56438, 118958, 33014, 108619, 68171, 50034, 31809, 9554, 108729, 24946, 24186, 8713, 125951, 32938, 78935, 13153, 110477, 21043, 112027, 48864, 124434, 113879, 121050, 3922, 112403, 125951, 21043, 56438, 112027, 9554, 108729, 113520, 34208, 99480, 76417, 3922, 56438, 9554, 112027, 108619, 68171, 48044, 58291, 43240, 19483, 125951, 41127, 13153, 3922, 125951, 122332, 68438, 125951, 17620, 117068, 116498, 116879, 125951, 110477, 15120, 121050, 68171, 103013, 249, 23187, 39013, 242, 5486, 102923, 108889, 32943, 34208, 105686, 110905, 35304, 10750, 24, 8107, 61075, 33671, 118664, 3490, 9080, 64209, 37687, 21043, 64467, 101402, 83175, 21043, 101402, 83175, 39276, 9554, 104356, 104587, 103179, 37687, 3922, 23039, 78519, 105885, 12774, 243, 101402, 83175, 117647, 47770, 9554, 121050, 1811, 103624, 121050, 76537, 105150, 35287, 42783, 55758, 126315, 64209, 37687, 103276, 28542, 3922, 112403, 127960, 64026, 103668, 110164, 121040, 9554, 104356, 1811, 9080, 64209, 127259, 118664, 30046, 21043, 103420, 102491, 73325, 14260, 105736, 101828, 103420, 102981, 19000, 845, 101083, 108142, 85707, 29391, 21405, 35287, 106616, 102700, 19967, 28038, 36827, 33014, 117238, 68931, 88343, 33443, 238, 43032, 35287, 101402, 83175, 39276, 23039, 78519, 105885, 12774, 243, 101402, 83175, 117238, 9554, 123123, 118187, 36827, 122411, 9554, 106246, 102210, 113433, 109098, 27384, 13647, 94, 112500, 1811, 128001]\n",
      "labels:\n",
      "细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
      "\n",
      "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|end_of_text|>\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:16:00.666669Z",
     "start_time": "2024-06-28T12:16:00.655930Z"
    }
   },
   "cell_type": "code",
   "source": "datasets[0].keys()",
   "id": "971820693cb805bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:06:03.500302Z",
     "start_time": "2024-06-28T12:06:03.488415Z"
    }
   },
   "cell_type": "code",
   "source": "len(datasets[0][\"input_ids\"])",
   "id": "730a4fcf18a99b67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:37:55.156933Z",
     "start_time": "2024-06-24T09:37:40.758619Z"
    }
   },
   "cell_type": "code",
   "source": "model = load_model(tokenizer[\"tokenizer\"], model_args, finetuning_args, is_trainable=True, add_valuehead=False)",
   "id": "e1ba568646769ff8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:731] 2024-06-24 17:37:40,761 >> loading configuration file /code-online/modelscope/llama3-chinese-Instruct/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-24 17:37:40,762 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/code-online/modelscope/llama3-chinese-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3471] 2024-06-24 17:37:40,765 >> loading weights file /code-online/modelscope/llama3-chinese-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-24 17:37:40,766 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-24 17:37:40,767 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2173837b5a04d9e9e1f23b146e18404"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-06-24 17:37:54,863 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-24 17:37:54,864 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /code-online/modelscope/llama3-chinese-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-06-24 17:37:54,870 >> loading configuration file /code-online/modelscope/llama3-chinese-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-24 17:37:54,871 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2024 17:37:54 - INFO - easyai.models.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/24/2024 17:37:54 - INFO - easyai.models.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/24/2024 17:37:54 - INFO - easyai.models.adapter - Upcasting trainable params to float32.\n",
      "06/24/2024 17:37:54 - INFO - easyai.models.adapter - Fine-tuning method: Full\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinetuning_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_valuehead\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/code-online/code/easy_ai/easyai/models/loader.py:177\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(tokenizer, model_args, finetuning_args, is_trainable, add_valuehead)\u001B[0m\n\u001B[1;32m    174\u001B[0m     patch_model(model, tokenizer, model_args, is_trainable, add_valuehead)\n\u001B[1;32m    175\u001B[0m     register_autoclass(config, model, tokenizer)\n\u001B[0;32m--> 177\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43minit_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinetuning_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m add_valuehead:\n\u001B[1;32m    180\u001B[0m     model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLMWithValueHead\u001B[38;5;241m.\u001B[39mfrom_pretrained(model)\n",
      "File \u001B[0;32m/code-online/code/easy_ai/easyai/models/adapter.py:367\u001B[0m, in \u001B[0;36minit_adapter\u001B[0;34m(config, model, model_args, finetuning_args, is_trainable)\u001B[0m\n\u001B[1;32m    364\u001B[0m     cast_trainable_params_to_fp32 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m finetuning_args\u001B[38;5;241m.\u001B[39mfinetuning_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 367\u001B[0m     \u001B[43m_setup_full_tuning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfinetuning_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_trainable_params_to_fp32\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m finetuning_args\u001B[38;5;241m.\u001B[39mfinetuning_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfreeze\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    375\u001B[0m     _setup_freeze_tuning(\n\u001B[1;32m    376\u001B[0m         model,\n\u001B[1;32m    377\u001B[0m         model_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    380\u001B[0m         cast_trainable_params_to_fp32,\n\u001B[1;32m    381\u001B[0m     )\n",
      "File \u001B[0;32m/code-online/code/easy_ai/easyai/models/adapter.py:56\u001B[0m, in \u001B[0;36m_setup_full_tuning\u001B[0;34m(model, model_args, finetuning_args, is_trainable, cast_trainable_params_to_fp32)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28many\u001B[39m(forbidden_module \u001B[38;5;129;01min\u001B[39;00m name \u001B[38;5;28;01mfor\u001B[39;00m forbidden_module \u001B[38;5;129;01min\u001B[39;00m forbidden_modules):\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cast_trainable_params_to_fp32:\n\u001B[0;32m---> 56\u001B[0m         param\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m     param\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU "
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:34:30.326532Z",
     "start_time": "2024-06-24T09:34:30.319088Z"
    }
   },
   "cell_type": "code",
   "source": "model_args\n",
   "id": "4d4db4a25a36358",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='/code-online/modelscope/llama3-chinese-Instruct', adapter_name_or_path=None, adapter_folder=None, cache_dir=None, use_fast_tokenizer=True, resize_vocab=False, split_special_tokens=False, new_special_tokens=None, model_revision='main', low_cpu_mem_usage=True, quantization_bit=None, quantization_type='nf4', double_quantization=True, quantization_device_map=None, rope_scaling=None, flash_attn='auto', shift_attn=False, mixture_of_depths=None, use_unsloth=False, visual_inputs=False, moe_aux_loss_coef=None, disable_gradient_checkpointing=False, upcast_layernorm=False, upcast_lmhead_output=False, train_from_scratch=False, infer_backend='huggingface', vllm_maxlen=2048, vllm_gpu_util=0.9, vllm_enforce_eager=False, vllm_max_lora_rank=32, offload_folder='offload', use_cache=True, infer_dtype='auto', hf_hub_token=None, ms_hub_token=None, export_dir=None, export_size=1, export_device='cpu', export_quantization_bit=None, export_quantization_dataset=None, export_quantization_nsamples=128, export_quantization_maxlen=1024, export_legacy_format=False, export_hub_model_id=None, print_param_status=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:34:42.497927Z",
     "start_time": "2024-06-24T09:34:42.488862Z"
    }
   },
   "cell_type": "code",
   "source": "finetuning_args",
   "id": "a8580de4d9c6c193",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinetuningArguments(use_badam=False, badam_mode='layer', badam_start_block=None, badam_switch_mode='ascending', badam_switch_interval=50, badam_update_ratio=0.05, badam_mask_mode='adjacent', badam_verbose=0, use_galore=False, galore_target=['all'], galore_rank=16, galore_update_interval=200, galore_scale=0.25, galore_proj_type='std', galore_layerwise=False, pref_beta=0.1, pref_ftx=0.0, pref_loss='sigmoid', dpo_label_smoothing=0.0, kto_chosen_weight=1.0, kto_rejected_weight=1.0, simpo_gamma=0.5, ppo_buffer_size=1, ppo_epochs=4, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_adapters=None, ref_model_quantization_bit=None, reward_model=None, reward_model_adapters=None, reward_model_quantization_bit=None, reward_model_type='lora', additional_target=None, lora_alpha=16, lora_dropout=0.0, lora_rank=8, lora_target=['all'], loraplus_lr_ratio=None, loraplus_lr_embedding=1e-06, use_rslora=False, use_dora=False, pissa_init=False, pissa_iter=4, pissa_convert=False, create_new_adapter=False, freeze_trainable_layers=2, freeze_trainable_modules=['all'], freeze_extra_modules=None, pure_bf16=False, stage='pt', finetuning_type='lora', use_llama_pro=False, freeze_vision_tower=True, train_mm_proj_only=False, plot_loss=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28034746ddc2877d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
